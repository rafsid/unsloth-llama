{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install triton --no-cache-dir","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -U accelerate --no-cache-dir","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\n# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n!pip install --no-deps \"trl<0.9.0\" peft accelerate bitsandbytes --no-cache-dir\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -U xformers --no-cache-dir","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install torch torchvision torchaudio --no-cache-dir","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -e .[triton] --no-cache-dir","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install wandb --upgrade --no-cache-dir","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nprint(torch.cuda.is_available())\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!nvcc --version\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !export OMP_NUM_THREADS=1\n# !export MKL_NUM_THREADS=1\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Unsloth","metadata":{}},{"cell_type":"code","source":"# import accelerate","metadata":{"execution":{"iopub.status.busy":"2024-06-12T19:28:48.559044Z","iopub.execute_input":"2024-06-12T19:28:48.559578Z","iopub.status.idle":"2024-06-12T19:28:48.563852Z","shell.execute_reply.started":"2024-06-12T19:28:48.559547Z","shell.execute_reply":"2024-06-12T19:28:48.563022Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nimport torch\nmax_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n\n# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\nfourbit_models = [\n    \"unsloth/mistral-7b-v0.3-bnb-4bit\",      # New Mistral v3 2x faster!\n    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n    \"unsloth/llama-3-8b-bnb-4bit\",           # Llama-3 15 trillion tokens model 2x faster!\n    \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n    \"unsloth/llama-3-70b-bnb-4bit\",\n    \"unsloth/Phi-3-mini-4k-instruct\",        # Phi-3 2x faster!\n    \"unsloth/Phi-3-medium-4k-instruct\",\n    \"unsloth/mistral-7b-bnb-4bit\",\n    \"unsloth/gemma-7b-bnb-4bit\",             # Gemma 2.2x faster!\n] # More models at https://huggingface.co/unsloth\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n    token = \"\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-12T19:28:48.568461Z","iopub.execute_input":"2024-06-12T19:28:48.568729Z","iopub.status.idle":"2024-06-12T19:29:04.666736Z","shell.execute_reply.started":"2024-06-12T19:28:48.568705Z","shell.execute_reply":"2024-06-12T19:29:04.665919Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","output_type":"stream"},{"name":"stderr","text":"2024-06-12 19:28:54.855146: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-12 19:28:54.855199: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-12 19:28:54.856807: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"==((====))==  Unsloth: Fast Llama patching release 2024.6\n   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\nO^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n\\        /    Bfloat16 = FALSE. Xformers = 0.0.26.post1. FA = False.\n \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n","output_type":"stream"},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"model = FastLanguageModel.get_peft_model(\n    model,\n    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n    lora_alpha = 16,\n    lora_dropout = 0, # Supports any, but = 0 is optimized\n    bias = \"none\",    # Supports any, but = \"none\" is optimized\n    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n    random_state = 3407,\n    use_rslora = False,  # We support rank stabilized LoRA\n    loftq_config = None, # And LoftQ\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T19:29:04.667921Z","iopub.execute_input":"2024-06-12T19:29:04.668446Z","iopub.status.idle":"2024-06-12T19:29:08.110551Z","shell.execute_reply.started":"2024-06-12T19:29:04.668418Z","shell.execute_reply":"2024-06-12T19:29:08.109779Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"Unsloth 2024.6 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n","output_type":"stream"}]},{"cell_type":"code","source":"alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\n{}\n\n### Input:\n{}\n\n### Response:\n{}\"\"\"\n\nEOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\ndef formatting_prompts_func(examples):\n    instructions = examples[\"instruction\"]\n    inputs       = examples[\"input\"]\n    outputs      = examples[\"output\"]\n    texts = []\n    for instruction, input, output in zip(instructions, inputs, outputs):\n        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n        texts.append(text)\n    return { \"text\" : texts, }\npass\n\nfrom datasets import load_dataset\ndataset = load_dataset(\"yahma/alpaca-cleaned\", split = \"train\")\ndataset = dataset.map(formatting_prompts_func, batched = True,)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T19:29:08.112982Z","iopub.execute_input":"2024-06-12T19:29:08.113280Z","iopub.status.idle":"2024-06-12T19:29:09.508975Z","shell.execute_reply.started":"2024-06-12T19:29:08.113254Z","shell.execute_reply":"2024-06-12T19:29:09.508215Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from trl import SFTTrainer\nfrom transformers import TrainingArguments\nfrom unsloth import is_bfloat16_supported\n\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = dataset,\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    dataset_num_proc = 2,\n    packing = False, # Can make training 5x faster for short sequences.\n    args = TrainingArguments(\n        per_device_train_batch_size = 2,\n        gradient_accumulation_steps = 4,\n        warmup_steps = 5,\n        max_steps = 2,\n        learning_rate = 2e-4,\n        fp16 = not is_bfloat16_supported(),\n        bf16 = is_bfloat16_supported(),\n        logging_steps = 1,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"linear\",\n        seed = 3407,\n        output_dir = \"outputs\",\n    ),\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T19:29:09.510261Z","iopub.execute_input":"2024-06-12T19:29:09.510553Z","iopub.status.idle":"2024-06-12T19:29:10.059580Z","shell.execute_reply.started":"2024-06-12T19:29:09.510527Z","shell.execute_reply":"2024-06-12T19:29:10.058689Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"max_steps is given, it will override any value given in num_train_epochs\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer_stats = trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-06-12T19:29:10.060603Z","iopub.execute_input":"2024-06-12T19:29:10.060845Z","iopub.status.idle":"2024-06-12T19:29:46.147994Z","shell.execute_reply.started":"2024-06-12T19:29:10.060823Z","shell.execute_reply":"2024-06-12T19:29:46.147238Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n   \\\\   /|    Num examples = 51,760 | Num Epochs = 1\nO^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n\\        /    Total batch size = 8 | Total steps = 2\n \"-____-\"     Number of trainable parameters = 41,943,040\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmdrafatsiddiqui\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240612_192914-r2tk3259</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mdrafatsiddiqui/huggingface/runs/r2tk3259' target=\"_blank\">outputs</a></strong> to <a href='https://wandb.ai/mdrafatsiddiqui/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mdrafatsiddiqui/huggingface' target=\"_blank\">https://wandb.ai/mdrafatsiddiqui/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mdrafatsiddiqui/huggingface/runs/r2tk3259' target=\"_blank\">https://wandb.ai/mdrafatsiddiqui/huggingface/runs/r2tk3259</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2/2 00:05, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.854700</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>2.270200</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"# alpaca_prompt = Copied from above\nFastLanguageModel.for_inference(model) # Enable native 2x faster inference\ninputs = tokenizer(\n[\n    alpaca_prompt.format(\n        \"Continue the fibonnaci sequence.\", # instruction\n        \"1, 1, 2, 3, 5, 8\", # input\n        \"\", # output - leave this blank for generation!\n    )\n], return_tensors = \"pt\").to(\"cuda\")\n\nfrom transformers import TextStreamer\ntext_streamer = TextStreamer(tokenizer)\n_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T19:29:46.149112Z","iopub.execute_input":"2024-06-12T19:29:46.149387Z","iopub.status.idle":"2024-06-12T19:29:55.234952Z","shell.execute_reply.started":"2024-06-12T19:29:46.149362Z","shell.execute_reply":"2024-06-12T19:29:55.233899Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nContinue the fibonnaci sequence.\n\n### Input:\n1, 1, 2, 3, 5, 8\n\n### Response:\n11, 13, 21, 34, 55, 89,... (and so on)\n\nThe Fibonacci sequence is a series of numbers in which each number is the sum of the two preceding numbers, starting from 0 and 1. The sequence begins with 1, 1, and each subsequent number is the sum of the previous two. The sequence continues indefinitely, with each number getting larger. The sequence is named after the Italian mathematician Leonardo Fibonacci, who introduced it in the 13th century as a solution to a problem involving the growth of a population of rabbits. The sequence has since been found to\n","output_type":"stream"}]},{"cell_type":"code","source":"if False:\n    from unsloth import FastLanguageModel\n    model, tokenizer = FastLanguageModel.from_pretrained(\n        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n        max_seq_length = max_seq_length,\n        dtype = dtype,\n        load_in_4bit = load_in_4bit,\n    )\n    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n\n# alpaca_prompt = You MUST copy from above!\n\ninputs = tokenizer(\n[\n    alpaca_prompt.format(\n        \"What is a famous tall tower in Paris?\", # instruction\n        \"\", # input\n        \"\", # output - leave this blank for generation!\n    )\n], return_tensors = \"pt\").to(\"cuda\")\n\noutputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\ntokenizer.batch_decode(outputs)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T19:29:55.236278Z","iopub.execute_input":"2024-06-12T19:29:55.236662Z","iopub.status.idle":"2024-06-12T19:29:59.564512Z","shell.execute_reply.started":"2024-06-12T19:29:55.236623Z","shell.execute_reply":"2024-06-12T19:29:59.563568Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"[\"<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWhat is a famous tall tower in Paris?\\n\\n### Input:\\n\\n\\n### Response:\\nThe Eiffel Tower is a famous tall tower in Paris. It was built for the 1889 World's Fair and stands at an impressive 324 meters (1,063 feet) tall. It is one of the most recognizable landmarks in the world and a symbol of French culture and engineering. The Eiffel\"]"},"metadata":{}}]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2024-06-12T19:29:59.565827Z","iopub.execute_input":"2024-06-12T19:29:59.566161Z","iopub.status.idle":"2024-06-12T19:30:00.652049Z","shell.execute_reply.started":"2024-06-12T19:29:59.566134Z","shell.execute_reply":"2024-06-12T19:30:00.650958Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Wed Jun 12 19:30:00 2024       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n| N/A   74C    P0              41W /  70W |   6503MiB / 15360MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n|   1  Tesla T4                       Off | 00000000:00:05.0 Off |                    0 |\n| N/A   55C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n+---------------------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\n\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"Number of GPUs: {torch.cuda.device_count()}\")\nfor i in range(torch.cuda.device_count()):\n    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-06-12T19:30:00.654136Z","iopub.execute_input":"2024-06-12T19:30:00.654540Z","iopub.status.idle":"2024-06-12T19:30:00.663010Z","shell.execute_reply.started":"2024-06-12T19:30:00.654500Z","shell.execute_reply":"2024-06-12T19:30:00.662015Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"CUDA available: True\nNumber of GPUs: 1\nGPU 0: Tesla T4\n","output_type":"stream"}]},{"cell_type":"code","source":"!echo $CUDA_VISIBLE_DEVICES\n","metadata":{"execution":{"iopub.status.busy":"2024-06-12T19:30:00.664816Z","iopub.execute_input":"2024-06-12T19:30:00.665184Z","iopub.status.idle":"2024-06-12T19:30:01.650087Z","shell.execute_reply.started":"2024-06-12T19:30:00.665147Z","shell.execute_reply":"2024-06-12T19:30:01.648808Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"0\n","output_type":"stream"}]},{"cell_type":"code","source":"#@title Show current memory stats\ngpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\nprint(f\"{start_gpu_memory} GB of memory reserved.\")","metadata":{"execution":{"iopub.status.busy":"2024-06-12T19:30:01.651659Z","iopub.execute_input":"2024-06-12T19:30:01.652003Z","iopub.status.idle":"2024-06-12T19:30:01.661485Z","shell.execute_reply.started":"2024-06-12T19:30:01.651972Z","shell.execute_reply":"2024-06-12T19:30:01.660372Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"GPU = Tesla T4. Max memory = 14.748 GB.\n6.561 GB of memory reserved.\n","output_type":"stream"}]},{"cell_type":"code","source":"#@title Show current memory stats\ngpu_stats = torch.cuda.get_device_properties(1)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\nprint(f\"{start_gpu_memory} GB of memory reserved.\")","metadata":{"execution":{"iopub.status.busy":"2024-06-12T19:30:01.665645Z","iopub.execute_input":"2024-06-12T19:30:01.665946Z","iopub.status.idle":"2024-06-12T19:30:02.525432Z","shell.execute_reply.started":"2024-06-12T19:30:01.665918Z","shell.execute_reply":"2024-06-12T19:30:02.523753Z"},"trusted":true},"execution_count":13,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#@title Show current memory stats\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m gpu_stats \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_device_properties\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m start_gpu_memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmax_memory_reserved() \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m      4\u001b[0m max_memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(gpu_stats\u001b[38;5;241m.\u001b[39mtotal_memory \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/cuda/__init__.py:447\u001b[0m, in \u001b[0;36mget_device_properties\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    445\u001b[0m device \u001b[38;5;241m=\u001b[39m _get_device_index(device, optional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m device_count():\n\u001b[0;32m--> 447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid device id\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _get_device_properties(device)\n","\u001b[0;31mAssertionError\u001b[0m: Invalid device id"],"ename":"AssertionError","evalue":"Invalid device id","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}Finetune LLM deploy
